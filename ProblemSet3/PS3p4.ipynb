{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fddc44eb-c70d-4364-a034-a43ec809f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import norm \n",
    "from scipy import stats\n",
    "import itertools\n",
    "from iminuit import Minuit   \n",
    "from scipy.optimize import minimize\n",
    "import sys  \n",
    "from importlib import reload\n",
    "from scipy import interpolate\n",
    "import copy\n",
    "\n",
    "# external libraries\n",
    "sys.path.append('../') \n",
    "import AdvAppStatFunctions as aas\n",
    "\n",
    "# setting for plotting\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-white')\n",
    "sns.set_style('white', {'legend.frameon':True})\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "sns.set_palette(\"colorblind\")\n",
    "color = sns.color_palette(\"colorblind\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4c4b758-0cd1-4442-a4f3-38a00d7a659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 15) (3000,)\n",
      "3000 training samples\n",
      "1500 validation samples\n",
      "1500 test samples\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test set\n",
    "X_train = np.genfromtxt('Set3_Prob4_TrainData.csv', skip_header=1, delimiter=',')[:,1:-1]\n",
    "Y_train = np.genfromtxt('Set3_Prob4_trainData.csv', skip_header=1, delimiter=',')[:,-1]\n",
    "print(X_train.shape, Y_train.shape)\n",
    "\n",
    "X_test = np.genfromtxt('Set3_Prob4_TestData.csv', skip_header=1, delimiter=',')[:,1:-1]\n",
    "Y_test = np.genfromtxt('Set3_Prob4_TestData.csv', skip_header=1, delimiter=',')[:,-1]\n",
    "\n",
    "# Further split test set into test and validation sets\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size=0.5)\n",
    "\n",
    "# Look at lengths of data\n",
    "n_train = len(X_train)\n",
    "n_val = len(X_val)\n",
    "n_test = len(X_test)\n",
    "print(f\"{n_train} training samples\")\n",
    "print(f\"{n_val} validation samples\")\n",
    "print(f\"{n_test} test samples\")\n",
    "\n",
    "# Convert to tensors\n",
    "#X_train = torch.as_tensor(X_train)\n",
    "#X_val = torch.as_tensor(X_val)\n",
    "#X_test = torch.as_tensor(X_test)\n",
    "#Y_train = torch.as_tensor(Y_train)\n",
    "#Y_val = torch.as_tensor(Y_val)\n",
    "#Y_test = torch.as_tensor(Y_test)\n",
    "\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "Y_train = torch.LongTensor(Y_train)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "Y_val = torch.LongTensor(Y_val)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "Y_test = torch.LongTensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8b819ad-578b-4bab-809d-1ddec0e0ffdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   0.,  18.,  ...,   2.,   1.,   0.],\n",
       "        [  0.,   0.,  53.,  ...,   3.,   0.,   0.],\n",
       "        [  0.,   0.,  17.,  ...,   2.,   0.,   0.],\n",
       "        ...,\n",
       "        [  0.,   0.,  54.,  ...,   3.,   0.,   0.],\n",
       "        [  0.,   0.,   2.,  ...,   4.,   0.,   1.],\n",
       "        [  0.,   0., 189.,  ...,   2.,   0.,   0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8f7c0-a836-4890-bc14-0d2b237fa3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00b8d620-af93-4db3-ba62-40e3efa3a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network contains 218 parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32a4a2ddf954b9d95c9e0d417d4899e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5c/kvpjh5n11_j2mp3l99c22xdr0000gn/T/ipykernel_61157/2263452946.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mpredicted_logprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#loss = loss_fn(predicted_logprobs, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_logprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# Make a simple neural network. We'll train this to output the *unnormalized* log-probabilities\n",
    "# that the input colors correspond to a galaxy and quasar respectively.\n",
    "hidden_dim = 8\n",
    "network = nn.Sequential(\n",
    "    nn.Linear(15, hidden_dim),  # The size of the input layer should match the number of features in your input dat\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, 2), # The size of the output layer should match the number of classes you're trying to predict\n",
    "    #nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "n_net_params = sum([p.numel() for p in network.parameters()])\n",
    "print(f\"Network contains {n_net_params} parameters\")\n",
    "\n",
    "# Set optimizer, loss and other training parameters\n",
    "optimizer = Adam(network.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "batch_size = 100 #100\n",
    "n_epochs = 15\n",
    "\n",
    "# For monitoring losses\n",
    "avg_epoch_losses_train = []\n",
    "avg_epoch_losses_val = [np.inf]\n",
    "pbar = trange(n_epochs)\n",
    "\n",
    "# Train!\n",
    "print(\"Training...\")\n",
    "for i in pbar:  # progress bar\n",
    "    # Training\n",
    "    optimizer.zero_grad()\n",
    "    network.train()\n",
    "    epoch_loss = 0\n",
    "    for x, y in zip(X_train[::batch_size], Y_train[::batch_size]):\n",
    "        predicted_logprobs = network(x)\n",
    "        #loss = loss_fn(predicted_logprobs, y)\n",
    "        loss = loss_fn(predicted_logprobs, torch.argmax(y, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_epoch_losses_train.append(epoch_loss / (n_train // batch_size))\n",
    "\n",
    "    # Validation\n",
    "    network.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(X_val[::batch_size], Y_val[::batch_size]):\n",
    "            predicted_logprobs = network(x)\n",
    "            loss = loss_fn(predicted_logprobs, y)\n",
    "            epoch_loss += loss.item()\n",
    "        avg_epoch_losses_val.append(epoch_loss / (n_val // batch_size))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f592c-a67d-45ac-a5bb-8e3873313cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e0978-1799-496c-9266-bbc727bfab93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
