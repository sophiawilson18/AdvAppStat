{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d40963-378d-4d9d-843d-b75725d09881",
   "metadata": {},
   "source": [
    "## Franks Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5364cf82-b5fa-4abb-a050-faaee674b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries \n",
    "import numpy as np       \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt                        \n",
    "from iminuit import Minuit                          \n",
    "import sys  \n",
    "\n",
    "# external libraries\n",
    "sys.path.append('../') \n",
    "import AdvAppStatFunctions as aas\n",
    "\n",
    "# setting for plotting\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['xtick.labelsize']=12\n",
    "plt.rcParams['ytick.labelsize']=12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b7603-13b9-437c-9797-616b4c7d13e0",
   "metadata": {},
   "source": [
    "### Parsing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "04732847-d47d-419c-b5b6-0049010a9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with help from Tania Kozynets' notebook\n",
    "\n",
    "# read data\n",
    "data = pd.read_table('FranksNumbers.txt', header=1, sep='\\s+', names = ['x','y','nan'])\n",
    "# sep='\\s+' defines the separator as being one single white space or more\n",
    "\n",
    "# remove last column since this is only nans\n",
    "data = data.drop(['nan'], axis=1)   #axis = 1 removes data in column\n",
    "\n",
    "# check if element in x-column is numeric. returns an array with true/false statements\n",
    "check_if_numeric = np.array([data['x'][i].isnumeric() for i in range(len(data))])\n",
    "\n",
    "# if element is not numeric, it must be the start of the header. now, select the rows with headers \n",
    "header_rows = np.where(check_if_numeric == False)[0]\n",
    "N_dataset = len(header_rows)+1\n",
    "\n",
    "# create array with all breaking rows (-1 to include the first header and len(data) to include the last row)\n",
    "breaking_rows = np.append(-1,header_rows)\n",
    "breaking_rows = np.append(breaking_rows,len(data))\n",
    "\n",
    "# storing the individual datasets in one big dictionary \n",
    "data_sorted = []\n",
    "\n",
    "for dataset_number in range(N_dataset):\n",
    "    data_sorted.append(np.array(data[breaking_rows[dataset_number]+1: \\\n",
    "                                  breaking_rows[dataset_number+1]].astype('float')))\n",
    "\n",
    "# backslash creates implicit line joining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ed532-2a1b-4af3-b61d-e2eb328571b2",
   "metadata": {},
   "source": [
    "### Calculate the mean and variance for each data set in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5c6ea2c6-4da0-4790-9919-383bef27fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(data, bias = False):\n",
    "    N = len(data)\n",
    "    if bias: return sum((data-np.average(data))**2) / N   # biased\n",
    "    else: return sum((data-np.average(data))**2) / (N-1)  # unbiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "0890f486-170a-4f4a-8eaa-a931a8c3d4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset 1 ---     \n",
      "Mean = 7.47     \n",
      "Biased variance = 3.75     \n",
      "Unbiased variance = 4.13 \n",
      "\n",
      "--- Dataset 2 ---     \n",
      "Mean = 8.07     \n",
      "Biased variance = 3.75     \n",
      "Unbiased variance = 4.13 \n",
      "\n",
      "--- Dataset 3 ---     \n",
      "Mean = 7.38     \n",
      "Biased variance = 3.75     \n",
      "Unbiased variance = 4.13 \n",
      "\n",
      "--- Dataset 4 ---     \n",
      "Mean = 6.88     \n",
      "Biased variance = 3.75     \n",
      "Unbiased variance = 4.13 \n",
      "\n",
      "--- Dataset 5 ---     \n",
      "Mean = 6.88     \n",
      "Biased variance = 3.75     \n",
      "Unbiased variance = 4.13 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_dataset):\n",
    "    n_dataset = i+1\n",
    "    mean = np.average(data_sorted[i][1], axis=0)\n",
    "    biased_variance = variance(data_sorted[0][:,1], bias = True)\n",
    "    unbiased_variance = variance(data_sorted[0][:,1])\n",
    "    \n",
    "    print(f'--- Dataset {n_dataset} --- \\\n",
    "    \\nMean = {mean:.2f} \\\n",
    "    \\nBiased variance = {biased_variance:.2f} \\\n",
    "    \\nUnbiased variance = {unbiased_variance:.2f} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5e006-e618-4025-92d7-b7a542c35a0c",
   "metadata": {},
   "source": [
    "### Using the eq. $y=x \\cdot 0.48 + 3.02$, calculate the Pearsonâ€™s $\\chi^2$ for each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b14d4625-c903-45e6-9e4b-051b1295097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical(x):\n",
    "    return x*0.48+3.02\n",
    "\n",
    "# extended dataset with the theoretical value in the third column\n",
    "data_extended = []\n",
    "\n",
    "for i in range(N_dataset): #0-4\n",
    "    dataset_extended = []\n",
    "    \n",
    "    for j in range(len(data_sorted[i][:,0])):\n",
    "        dataset_extended.append(np.append(data_sorted[i][j], theoretical(data_sorted[i][j][0])).tolist())\n",
    "        \n",
    "    data_extended.append(np.array(dataset_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "06ed19dc-e590-4069-8018-0052362527f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square(y_exp, y_obs, y_err):\n",
    "    return np.sum((y_exp-y_obs)**2/y_err**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "b91086ff-890d-4f9b-bd15-cb176f271f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_sqrt_err = []\n",
    "chi2_fixed_err = []\n",
    "\n",
    "for i in range(N_dataset):\n",
    "    y_exp = data_extended[i][:,2]\n",
    "    y_obs = data_extended[i][:,1]\n",
    "     \n",
    "    # uncertainty is the squareroot of the expected value\n",
    "    y_err = np.sqrt(y_exp)  \n",
    "    chi2_sqrt_err.append(chi_square(y_exp, y_obs, y_err))\n",
    "    \n",
    "    # uncertainty is the squareroot of the expected value\n",
    "    y_err = 1.22\n",
    "    chi2_fixed_err.append(chi_square(y_exp, y_obs, y_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "8b36dd7e-6019-4af3-ae1f-0b0751c10a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset 1 ---     \n",
      "Chi2 (w/ pm sqrt uncertainty) \\= 1.887     \n",
      "Chi2 (w/ pm 1.22 uncertainty) = 9.468 \n",
      "\n",
      "--- Dataset 2 ---     \n",
      "Chi2 (w/ pm sqrt uncertainty) \\= 2.072     \n",
      "Chi2 (w/ pm 1.22 uncertainty) = 9.477 \n",
      "\n",
      "--- Dataset 3 ---     \n",
      "Chi2 (w/ pm sqrt uncertainty) \\= 1.555     \n",
      "Chi2 (w/ pm 1.22 uncertainty) = 9.460 \n",
      "\n",
      "--- Dataset 4 ---     \n",
      "Chi2 (w/ pm sqrt uncertainty) \\= 2.043     \n",
      "Chi2 (w/ pm 1.22 uncertainty) = 9.454 \n",
      "\n",
      "--- Dataset 5 ---     \n",
      "Chi2 (w/ pm sqrt uncertainty) \\= 7.556     \n",
      "Chi2 (w/ pm 1.22 uncertainty) = 37.858 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_dataset):\n",
    "    n_dataset = i+1\n",
    "    \n",
    "    print(f'--- Dataset {n_dataset} --- \\\n",
    "    \\nChi2 (w/ pm sqrt uncertainty) \\= {chi2_sqrt_err[i]:.3f} \\\n",
    "    \\nChi2 (w/ pm 1.22 uncertainty) = {chi2_fixed_err[i]:.3f} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26077872-c159-4c27-b06a-4f51b305b5b9",
   "metadata": {},
   "source": [
    "The sqrt(y_exp) - uncertainty shows better agreement with the data since this uncertainty is larger than 1.22. However, since N is small in all five data sets this is a bad way to estimate the uncertainty. This is only ok when N is large. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
